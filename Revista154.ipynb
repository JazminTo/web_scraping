{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9badd40a-6ada-47e7-b0f7-3d60a73c4d2f",
   "metadata": {},
   "source": [
    "Importacion de librerias.\n",
    "\n",
    "Requests realiza la peticion al servidor.\n",
    "\n",
    "BeautifulSoup analizar documentos HTML.\n",
    "\n",
    "Pandas podemos representar datos tabulares con columnas con etiquetas y filas y series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900e5a1-ca6e-4ad9-8424-51f5c84e7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30492a2c-7db5-4cb2-a83d-c7d5d956c60d",
   "metadata": {},
   "source": [
    "Se reliza un request de la url_inicial de la revista, url_root nos ayuda a genera un url completa a partir de la url raiz para esto se utiliza la libreria urljoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce0a93-69a1-4211-9071-039c246b86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial='http://www.revistarelaciones.com/index.php/relaciones/issue/archive'\n",
    "url_root= 'http://www.revistarelaciones.com/index.php/relaciones/issue/archive'\n",
    "r = requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbad1ef-5a26-4577-97a4-1bfad0686abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee54b8-95f6-414d-befe-76ecc3328656",
   "metadata": {},
   "source": [
    "Se realiza la busqueda para obtener los urls de cada uno de los volumenes o archivos de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7c8a3-5a44-41ca-a38f-38dea941f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = soup.find('div', id='issues')\n",
    "volumen=box.findAll('h4')\n",
    "vol = [x.find('a').get('href')for x in volumen]\n",
    "#vol=[urljoin(url_root,i) for i in vol]\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d403f1c-6029-4d3a-8069-0d6cb138443d",
   "metadata": {},
   "source": [
    "Se realiza un lista en la que la variable vol que contiene las urls de cada uno de \n",
    "los volumenes publicados de la revista, con estos urls obtenidos,  con la variable vol se implementa un ciclo en el cual se utiliza la variable vol2 de la cual se van acumulando las siguientes urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65c18e-893b-4f93-956d-5a5dd88aee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.findAll('div', id='issueCoverImage') \n",
    "    #volumen1=box1.findAll('h3', class_='title')\n",
    "    vol1 = [x.find('a').get('href')for x in box1]\n",
    "    #vol1=[urljoin(url_root,p) for p in vol1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00aa7a-54ee-49e4-a1fe-33a2070c0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc61ab0-6048-46c6-8a5d-edeb2735b512",
   "metadata": {},
   "source": [
    "Algunas revista nececita hacer mas busque da para llega a los articulos\n",
    "ya que algunos maneja primero los volumens o achivos y de eso achivos se dirigen\n",
    "a un urls que es la tabla contendios, donde se encuentran los articulos,por lo cual se comieza a realizar la misma busqueda que en vol2.\n",
    "\n",
    "Se implementa otro ciclo con vol2, para esto se utiliza la variable de vol3 que recolecta las urls de los articulos, para comenzar con su scrapeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34755ef2-c26e-4a6c-96d0-f8a505d5c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3 =[]\n",
    "for b in vol2: \n",
    "    url_inicial2=b \n",
    "    r2 = requests.get(url_inicial2)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('tr', valign='top')\n",
    "    vol2= [o.find('a').get('href')for o in volumen2]\n",
    "    #links = [o.find('a')['href'] for o in box2.findAll('div',class_='tocTitle')]\n",
    "    #vol1=[urljoin(url_root,p) for p in vol1]\n",
    "    vol3+=vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffbddb-3d33-4969-853b-ecaa57903263",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a440f-3c5e-44ab-8277-f66fb55f7d56",
   "metadata": {},
   "source": [
    "En esta funcion variable sopa se utiliza para tener las paginaciones de la revista y la variable url es guardan cada uno de url que ya se obtivieron de vol3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1094410-ffeb-4cba-9a32-3ee9020283ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_items(sopa, url): \n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('tr', valign='top')\n",
    "    vol2= [o.find('a').get('href')for o in volumen2]\n",
    "    #vol=[urljoin(url_root,i) for i in vol]\n",
    "    return vol3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c0912-0c6e-4a31-9e93-93387870746f",
   "metadata": {},
   "source": [
    "Ahora se va acumulando cada uno de lo links e ir iterando en cada una\n",
    "de las páginas, para traer cada unos de los links que se van a ir scrapeando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5a581-e926-410f-8bc7-5e0f03c79669",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break\n",
    "    url_inicial = urljoin(url_inicial, next_a[0].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d918b43-1390-4f41-b94f-feadce864eb8",
   "metadata": {},
   "source": [
    "Obtiene el numero de los articulos que se enontraron en los primeros volumenes de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4ee8a-26ef-4c75-a2e3-9da2d02df0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90219ef3-d089-44e5-b42b-96cf789f9c16",
   "metadata": {},
   "source": [
    "Toma uno a uno de los links donde se encuantra la informacion de cada articulo para Scrapearlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41155c42-668a-4178-b7ac-88d8f191713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f18c75-72bb-4adc-8605-f8f681b91edd",
   "metadata": {},
   "source": [
    "Mediante esta función resivisa cada uno de los links de los articulos en los cuales se encunntra la información\n",
    "del articulo, por lo cual se aplican  cada uno de los metodos, que nos van a obtener lo que estamos requiriendo de dicho articulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e6ce8-3034-4e6c-992e-0aec9071baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_book(url):\n",
    "    content_book={}# diccionario que no va acumulando la imformacion que vamos poteniddo para el tada frame\n",
    "    r=requests.get(url)\n",
    "    tr='Relaciones Estudios de Historia y Sociedad'\n",
    "    a='IV. Humanidades y Ciencias de la Conducta'\n",
    "    te='Multidisciplinaria'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    tituloR=tr\n",
    "    if tituloR:\n",
    "        content_book['Titulo Revista']=tituloR\n",
    "    else :\n",
    "        content_book['Titulo Revista']=None\n",
    "    area=a\n",
    "    if area:\n",
    "        content_book['Area']=area\n",
    "    else :\n",
    "        content_book['Area']=None\n",
    "    tematica=te\n",
    "    if tematica:\n",
    "        content_book['Tematica']=tematica\n",
    "    else :\n",
    "        content_book['Tematica']=None\n",
    "    #titulo del libro\n",
    "    try:\n",
    "        titulo=s_item.find('h3').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titulo\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    #obtencion de Resumen\n",
    "    try:\n",
    "        des=s_item.find('div', id='articleAbstract').get_text(strip=True)\n",
    "        content_book['Resumen']=des.replace(\"Resumen\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    #obtencion de Abstract\n",
    "    try:\n",
    "        de=s_item.find('div', class_='panel-boody').get_text(strip=True)\n",
    "        content_book['Abstract']=de#.replace(\"Abstract\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    #obtener link Articulos con la url, donde se guarda los urls donde se encutran la informacion de los articulos.\n",
    "    try:\n",
    "        urlA=url\n",
    "        content_book['Link Articulo']=urlA\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "    #obtener link PDF\n",
    "    try:\n",
    "        ancla_link=s_item.find('div', id='articleFullText').find('a').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root,ancla_link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45838b32-d861-4664-b8bd-73d205390759",
   "metadata": {},
   "source": [
    "El list_scraper hace un scrapeo de la cada uno de los articulos, ya que en datos_book se acumulo la informacion obtenida de lo metodos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569be7e-3fcd-47da-b228-e98b63745982",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=list_scraper[0:588]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fb84d-9df5-4e7b-a761-c9e2ec05fdf9",
   "metadata": {},
   "source": [
    "La variable datos_book es un listado en el que se recolecto la imformacion requerida por lo cual\n",
    "se convierte en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2dc6e2-000a-4dfc-81f7-263a6e90e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d4989-2e43-439d-a2c0-34de2281e9be",
   "metadata": {},
   "source": [
    "En el caso de esta revista solo de deseaba obtener los articulos, por lo cual\n",
    "en algunas ocaciones se obtienes cosas diferentes, para esto con la funcion isin lo que hace es \n",
    "eliminar mediante el Titulo Articulo los archivos que no se desean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374afb68-34bc-46db-98c3-efd30f52c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_catalogo[~ df_catalogo[\"Titulo Articulo\"].isin([\"Presentación\",\"Editorial\",\"Autores\",\"Introducción\",\"Corrigenda\",\"Comentario\",\"Revista completa\",\"Preliminares\",\"Portada e índice\",\"Enlaces refback\",\"Mañana o pasado\",\"Pura imagen\",\"Número completo\", None])]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a9d15-294c-4b40-b658-d86b5bc76d43",
   "metadata": {},
   "source": [
    "La imformacion solicitada se pasa a un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10050de8-6fbd-44e7-81c3-6ba892de16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Revista154.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4a471-981d-44af-accf-5b557b27d06a",
   "metadata": {},
   "source": [
    "Final del Código"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

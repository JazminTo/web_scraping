{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe522eb-3664-4ce0-b7e4-69d23d22eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerias\n",
    "#peticion al servidor\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce0a93-69a1-4211-9071-039c246b86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial='http://www.revistarelaciones.com/index.php/relaciones/issue/archive'\n",
    "url_root= 'http://www.revistarelaciones.com/index.php/relaciones/issue/archive'\n",
    "r = requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbad1ef-5a26-4577-97a4-1bfad0686abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca37a6-d926-4d89-8ac0-87da8c58f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se realiza la busquea de la primeras etiquetas\n",
    "#para obtener lo href de cada uno de lo volumenes o archivos de la revista.\n",
    "box = soup.find('div', id='issues')\n",
    "volumen=box.findAll('h4')\n",
    "vol = [x.find('a').get('href')for x in volumen]\n",
    "#vol=[urljoin(url_root,i) for i in vol]\n",
    "vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39448ce-67d4-43f0-b39f-6e3ec05908b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se realiza un lista en la que la variavle vol contiene los principales href\n",
    "#con estos href obtenidos, se implemeta un ciclo de esto para con ello obtener\n",
    "#los siguentes href de los arcticulos que se encuentran en cada uno de los volumenes o archivos\n",
    "#para esto se realiza la misma busqueda que al principio.\n",
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.findAll('div', id='issueCoverImage') \n",
    "    #volumen1=box1.findAll('h3', class_='title')\n",
    "    vol1 = [x.find('a').get('href')for x in box1]\n",
    "    #vol1=[urljoin(url_root,p) for p in vol1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00aa7a-54ee-49e4-a1fe-33a2070c0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a6d80-8cc8-4be2-8b28-0d28896cd578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algunas revista nececita hacer mas busque da para llega a los articulos\n",
    "#ya que algunos maneja primero los volumens o achivos y de eso achivos se dirigen\n",
    "#a un href que es la tabla contendios, donde se encuentran los articulos.\n",
    "#por lo cual se comieza a realizar la misma buequeda que en vol2.\n",
    "vol3 =[]\n",
    "for b in vol2: \n",
    "    url_inicial2=b \n",
    "    r2 = requests.get(url_inicial2)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('tr', valign='top')\n",
    "    vol2= [o.find('a').get('href')for o in volumen2]\n",
    "    #links = [o.find('a')['href'] for o in box2.findAll('div',class_='tocTitle')]\n",
    "    #vol1=[urljoin(url_root,p) for p in vol1]\n",
    "    vol3+=vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffbddb-3d33-4969-853b-ecaa57903263",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1094410-ffeb-4cba-9a32-3ee9020283ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En esta parte la funcion es de obtener links\n",
    "def get_url_items(sopa, url): \n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('tr', valign='top')\n",
    "    vol2= [o.find('a').get('href')for o in volumen2]\n",
    "    #vol=[urljoin(url_root,i) for i in vol]\n",
    "    return vol3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8b8c6-f5ae-4d13-8966-a5431884bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora se va acumulando cada uno de lo links e ir iterando en cada una\n",
    "#de las páginas, para traer cada unos de los links que se van a ir scrapeando.\n",
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break\n",
    "    url_inicial = urljoin(url_inicial, next_a[0].get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613948d-eb8f-4fb4-8938-410fdee067a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se hace un listado de los links de lo articulos que se obtuvieron.\n",
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93956633-116d-4199-b459-a1af1bed2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se estaran tomado uno a uno los links para Scrapearlos.\n",
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da03e9-4678-45ef-8059-81a4875dc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mediante esta función resivi cada uno de los links, va ir aplicando caca uno de \n",
    "#los metodos, que nos van a obtener lo que estamos requiriendo.\n",
    "def scraper_book(url):\n",
    "    content_book={}# diccionario que no va acumulando la imformacion que vamos poteniddo para el tada frame\n",
    "    r=requests.get(url)\n",
    "    tr='Relaciones Estudios de Historia y Sociedad'\n",
    "    a='IV. Humanidades y Ciencias de la Conducta'\n",
    "    te='Multidisciplinaria'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    tituloR=tr\n",
    "    if tituloR:\n",
    "        content_book['Titulo Revista']=tituloR\n",
    "    else :\n",
    "        content_book['Titulo Revista']=None\n",
    "    area=a\n",
    "    if area:\n",
    "        content_book['Area']=area\n",
    "    else :\n",
    "        content_book['Area']=None\n",
    "    tematica=te\n",
    "    if tematica:\n",
    "        content_book['Tematica']=tematica\n",
    "    else :\n",
    "        content_book['Tematica']=None\n",
    "    #titulo del libro\n",
    "    try:\n",
    "        titulo=s_item.find('h3').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titulo\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    #obtencion de Resumen\n",
    "    try:\n",
    "        des=s_item.find('div', id='articleAbstract').get_text(strip=True)\n",
    "        content_book['Resumen']=des.replace(\"Resumen\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    #obtencion de Abstract\n",
    "    try:\n",
    "        de=s_item.find('div', class_='panel-boody').get_text(strip=True)\n",
    "        content_book['Abstract']=de#.replace(\"Abstract\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    #obtener link Articulos con la url, donde se guarda lo href\n",
    "    #donde se encutran la informacion de los articulos.\n",
    "    try:\n",
    "        urlA=url\n",
    "        content_book['Link Articulo']=urlA\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "    #obtener link PDF\n",
    "    try:\n",
    "        ancla_link=s_item.find('div', id='articleFullText').find('a').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root,ancla_link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b38714-cab7-4584-8234-cd02f16581ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#El ciclo hace un scrapeo de la cada uno de los articulos.\n",
    "list_scraper=list_scraper[0:588]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa8a0e-b02b-4722-8788-4fc362c877dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En listado de datos_book que se obtuvo, se guarada en una variable\n",
    "#y dicha variable que conteien la imformacion se convierte en un DataFrame.\n",
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d8c48-add1-42c2-b348-9e412528968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En el caso de esta revista solo de deseaba obtener los articulos,por lo cual\n",
    "#en algunas ocaciones se obtenia cosa diferntes, por lo cual esta funcion lo que hace es \n",
    "#eliminar mediante el Tiitulo Articulo lo que no se desea.\n",
    "df=df_catalogo[~ df_catalogo[\"Titulo Articulo\"].isin([\"Presentación\",\"Editorial\",\"Autores\",\"Introducción\",\"Corrigenda\",\"Comentario\",\"Revista completa\",\"Preliminares\",\"Portada e índice\",\"Enlaces refback\",\"Mañana o pasado\",\"Pura imagen\",\"Número completo\", None])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451b097-3568-44c6-a8af-8899c24669ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La imformacion conseguda se pasa a un CSV.\n",
    "df.to_csv('Revista154.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4a471-981d-44af-accf-5b557b27d06a",
   "metadata": {},
   "source": [
    "Final del Código"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

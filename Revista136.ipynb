{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b3113d-e858-4484-8b6f-de75466dee8f",
   "metadata": {},
   "source": [
    "# WEB SCRAPING\n",
    "  \n",
    "Es una técnica para extraer y almacenar datos de una o varias páginas web con el fin de analizarlos o manipularlos en otros medios, para la cual se utilizan bots para extraer los datos y contenidos de las webs. \n",
    "\n",
    "### Importación de librerías.\n",
    "\n",
    "* Requests realiza la petición al servidor.\n",
    "\n",
    "* BeautifulSoup analizar documentos HTML.\n",
    "\n",
    "* Pandas podemos representar datos tabulares con columnas con etiquetas y filas y series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb984c-c500-4977-b029-a514fb1099b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6361ea-31ab-406a-99a6-c3f50d9e7083",
   "metadata": {},
   "source": [
    "Se realiza un request de la url_inicial de la revista, url_root nos ayuda a genera un url completa a partir de la url raíz para esto se utiliza la librería urljoin.\n",
    "\n",
    "La revista que se está scrapendo es la siguiente http://www.revistas.unam.mx/front/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169595ee-e03c-441a-be88-24978a2bc548",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial='http://revistas.unam.mx/index.php/entreciencias/issue/archive'\n",
    "url_root= 'http://revistas.unam.mx/index.php/entreciencias/issue/archive'\n",
    "r = requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04be29-16b1-4abb-b984-22eea3ebea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed8125-03f2-4977-aa5c-704538de3f54",
   "metadata": {},
   "source": [
    "### Obtención de los primeros volúmenes.\n",
    "\n",
    "Se realiza la búsqueda para obtener los urls de cada uno de los volúmenes o archivos de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdaa2f8-4e6e-4f5d-9766-8cf950f0f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = soup.find('div', id='issues')\n",
    "volu=box.findAll('h4')\n",
    "vol = [x.find('a').get('href')for x in volu]\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178149a-320e-4d77-9617-6a9afc515aaf",
   "metadata": {},
   "source": [
    "### Obtención de los segundos urls.\n",
    "\n",
    "Se realiza una lista en la que la variable *vol* que contiene las urls de cada uno de los volúmenes publicados de la revista, con estos urls obtenidos,  con la variable *vol* se implementa un ciclo en el cual se utiliza la variable *vol2* de la cual se van acumulando las siguientes urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af501f1-117b-4a33-bd39-0b5b0fa3c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.findAll('div', id='issueCoverImage') \n",
    "    #volumen1=box1.findAll('h3',class_='title')\n",
    "    vol1 = [x.find('a').get('href')for x in box1]\n",
    "    #vol1=[urljoin(url_root,p) for p in vol1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c25a4b-f7b7-4862-8286-5148a9832dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3fb04d-4a75-43d2-bd43-1474dbdb6421",
   "metadata": {},
   "source": [
    "### Obtención de las ulrs de los artículos.\n",
    "\n",
    "Algunas revistas necesitan más búsquedas para llega a los artículos ya que algunos manejan primero los volúmenes o archivos y de eso archivos se dirigen a un urls que es la tabla contenidos, donde se encuentran los artículos, por lo cual se comienza a realizar la misma búsqueda que en *vol2*.\n",
    "\n",
    "Se implementa otro ciclo con *vol2*, para esto se utiliza la variable de *vol3* que recolecta las urls de los artículos, para comenzar con su scrapeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e96cad-87a0-4ca9-9532-31f78b8b1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3 =[]\n",
    "for b in vol2: \n",
    "    url_inicial2=b \n",
    "    r2 = requests.get(url_inicial2)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('div',class_='tocTitle')\n",
    "    vol2 = [o.find('a').get('href')for o in volumen2]\n",
    "    #vol1=[urljoin(url_root,p) for p in vol1]\n",
    "    vol3+=vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15c27d-c2b5-49a5-8460-9aaff9368caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d9d4b-18dc-4136-a6ee-80a5d62de0e9",
   "metadata": {},
   "source": [
    "En esta función variable sopa se utiliza para tener las paginaciones de la revista y la variable url es guardan cada uno de url que ya se obtuvieron de *vol3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02c3a3-6f1c-4473-8ee6-c51467654e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_items(sopa, url): \n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('div',class_='tocTitle')\n",
    "    vol2 = [x.find('a').get('href')for x in volumen2]\n",
    "    #vol=[urljoin(url_root,i) for i in vol]\n",
    "    return vol3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dc6c0-543c-4230-a966-672ce54c687a",
   "metadata": {},
   "source": [
    "Ahora se va acumulando cada uno de los links e ir iterando en cada una de las páginas, para traer cada uno de los links que se van a ir scrapeando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163198f-9312-4014-b8e7-3b46b902b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break\n",
    "    url_inicial = urljoin(url_inicial, next_a[0].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae642f3-e3c0-4769-a5c7-8a5642453ae1",
   "metadata": {},
   "source": [
    "Obtiene el número de los artículos que se encontraron en los primeros volúmenes de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babbc819-84f3-4813-b5e1-6b229d3ea155",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1815211-2117-4581-ab7f-8e60c90792bb",
   "metadata": {},
   "source": [
    "Toma uno a uno de los links donde se encuentra la información de cada artículo para Scrapearlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a1fe7-b12c-4046-a17a-a615ef83e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7649a-7d82-4e08-9040-5431d254885c",
   "metadata": {},
   "source": [
    "### Scrapeo de las urls de los ariculos.\n",
    "\n",
    "Mediante esta función revisa cada uno de los links de los artículos en los cuales se encuentra la información\n",
    "del artículo, por lo cual se aplican cada uno de los métodos, que nos van a obtener lo que estamos requiriendo de dicho artículo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b79918-f803-4b51-8711-f64fd7897e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_book(url):\n",
    "    content_book={}\n",
    "    r=requests.get(url)\n",
    "    tr='Entreciencias: Diálogos en la Sociedad del Conocimiento'\n",
    "    a='Multidisciplinaria'\n",
    "    te='Multidisciplinaria'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    tituloR=tr\n",
    "    if tituloR:\n",
    "        content_book['Titulo Revista']=tituloR\n",
    "    else :\n",
    "        content_book['Titulo Revista']=None\n",
    "    area=a\n",
    "    if area:\n",
    "        content_book['Area']=area\n",
    "    else :\n",
    "        content_book['Area']=None\n",
    "    tematica=te\n",
    "    if tematica:\n",
    "        content_book['Tematica']=tematica\n",
    "    else :\n",
    "        content_book['Tematica']=None\n",
    "    #titulo del libro\n",
    "    try:\n",
    "        titulo=s_item.find('h3').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titulo\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    try:\n",
    "        des=s_item.find('div', id='articleAbstract').get_text(strip=True)\n",
    "        content_book['Resumen']=des.replace(\"Resumen\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    try:\n",
    "        de=s_item.find('div', class_='panel-boody').get_text(strip=True)\n",
    "        content_book['Abstract']=de#.replace(\"Abstract\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    try:\n",
    "        urlA=url\n",
    "        content_book['Link Articulo']=urlA\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "        #obtener link PDF\n",
    "    try:\n",
    "        ancla_link=s_item.find('div', id='articleFullText').find('a').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root,ancla_link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791d4aa-14df-4a7e-823d-cd17e2ec6b2f",
   "metadata": {},
   "source": [
    "El list_scraper hace un scrapeo de la cada uno de los artículos, ya que en datos_book se acumuló la información obtenida de lo métodos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2bde0-64f7-4b8b-9b5e-81656b3f4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=list_scraper[0:230]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbf10b-42ed-4ee4-84e2-a1f219e3f1dd",
   "metadata": {},
   "source": [
    "### Se obtiene la información requerida de los artículos.\n",
    "\n",
    "La variable **datos_book** es un listado en el que se recolecto la información requerida por lo cual\n",
    "se convierte en un **DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58979d17-d67c-4652-8d51-f50d0f5899c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2080d-90a0-4c3b-936d-969fb8f820e8",
   "metadata": {},
   "source": [
    "En el caso de esta revista solo de deseaba obtener los artículos, por lo cual\n",
    "en algunas ocasiones se obtienes cosas diferentes, para esto con la función isin lo que hace es \n",
    "eliminar mediante el **Titulo Articulo** los archivos que no se desean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1df0e0-91cd-447e-be5b-964d55d8d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_catalogo[~ df_catalogo[\"Titulo Articulo\"].isin([\"Presentación\",\"Editorial\",\"Autores\",\"Introducción\",\"Corrigenda\",\"Comentario\",\"Revista completa\",\"Preliminares\",\"Portada e índice\",\"Letter Submission\",\"Carta de presentación de manuscritos\",\"In memoriam\",\"COVER\",\"Code of Ethics\",\"CODIGO DE ÉTICA\",\"EDITORIAL PROCESS\",\"In Memoriam\",\"Complate journal\",\"Preliminary\",\"Preliminaries\",\"Complete Journal\",\"Indización y evaluación de Nova Scientia\",\"Editorial del Décimoprimer Número\",\"Nota Editorial\"])]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c3d12-d0fc-4035-a222-b1ceb357d082",
   "metadata": {},
   "source": [
    "La información solicitada se pasa a un **CSV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fb020-b981-4c14-9710-74a8a3e6d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Revista136.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32572f20-95b6-4c31-9bee-6e65a7b477d6",
   "metadata": {},
   "source": [
    "Final del Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c98e7-8890-4420-a9cb-2ede11fd89c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

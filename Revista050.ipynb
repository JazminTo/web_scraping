{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936bae0f-d264-4a73-ae54-17135af7e66f",
   "metadata": {},
   "source": [
    "# WEB SCRAPING\n",
    "  \n",
    "Es una técnica para extraer y almacenar datos de una o varias páginas web con el fin de analizarlos o manipularlos en otros medios, para la cual se utilizan bots para extraer los datos y contenidos de las webs. \n",
    "\n",
    "### Importación de librerías.\n",
    "\n",
    "* Requests realiza la petición al servidor.\n",
    "\n",
    "* BeautifulSoup analizar documentos HTML.\n",
    "\n",
    "* Pandas podemos representar datos tabulares con columnas con etiquetas y filas y series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44dc69-654d-4ffc-9a96-c78e7c49a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417015c8-a2d1-4abb-8dc7-3fe777060ce2",
   "metadata": {},
   "source": [
    "Se realiza un request de la url_inicial de la revista, url_root nos ayuda a genera un url completa a partir de la url raíz para esto se utiliza la librería urljoin.\n",
    "\n",
    "La revista que se está scrapendo es la siguiente http://www.revistas.unam.mx/front/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f2a1c-cc8d-40ff-b2ea-310fc9c353b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial='http://www.revistas.unam.mx/index.php/rmac/issue/archive?issuesPage=4#issues'\n",
    "url_root= 'http://www.revistas.unam.mx/index.php/rmac/issue/archive?issuesPage=4#issues'\n",
    "r = requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa1391-dab4-4f91-86d1-bd7c0bde54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370acc8-cda5-4166-af44-d1b1784dc012",
   "metadata": {},
   "source": [
    "## Obtención de los primeros volúmenes.\n",
    "\n",
    "Se realiza la búsqueda para obtener los urls de cada uno de los volúmenes o archivos de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb3b24-e1bd-49b2-9dd9-875bc748ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = soup.find('div', id='issues')\n",
    "volumen=box.findAll('h4')\n",
    "vol = [x.find('a').get('href')for x in volumen]\n",
    "vol=[urljoin(url_root,i) for i in vol]\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40dfb0a-530b-4ef2-9a53-821c573864cf",
   "metadata": {},
   "source": [
    "### Obtención de los segundos urls.\n",
    "\n",
    "Se realiza una lista en la que la variable *vol* que contiene las urls de cada uno de los volúmenes publicados de la revista, con estos urls obtenidos,  con la variable *vol* se implementa un ciclo en el cual se utiliza la variable *vol2* de la cual se van acumulando las siguientes urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc18ca-dc0b-4995-8658-a6d51e7d5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.find('div', id='content') \n",
    "    volumen1=box1.findAll('a')\n",
    "    vol1 =[x.get('href')for x in volumen1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbbd47-0e68-4209-985a-44393855d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4121a-ac0e-4437-b93b-655d6a154178",
   "metadata": {},
   "source": [
    "En esta función variable sopa se utiliza para tener las paginaciones de la revista y la variable url es guardan cada uno de url que ya se obtuvieron de *vol2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9ecd3-820e-4861-b64e-f78e0a4b0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_items(sopa, url): \n",
    "    box1 = soup1.find('div', id='content') \n",
    "    volumen1=box1.findAll('a')\n",
    "    vol1 =[x.get('href')for x in volumen1]\n",
    "    return vol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac8c66-f872-42a2-9ab9-d08f19f6f9b1",
   "metadata": {},
   "source": [
    "Ahora se va acumulando cada uno de los links e ir iterando en cada una de las páginas, para traer cada uno de los links que se van a ir scrapeando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f435f5-997d-4412-a30c-7aca1d47fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break\n",
    "    url_inicial = urljoin(url_inicial, next_a[0].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890be34-6a13-45e5-8007-d295a043839a",
   "metadata": {},
   "source": [
    "Obtiene el número de los artículos que se encontraron en los primeros volúmenes de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f09c92-6dc1-461c-9007-d0cea02e140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b3ed0d-aea6-48d7-ae2b-5c56777eb747",
   "metadata": {},
   "source": [
    "Toma uno a uno de los links donde se encuentra la información de cada artículo para Scrapearlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6b777-274f-42d1-8e34-01d0c491e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59abc3-cf1f-4dab-a8b2-5915c9e787fa",
   "metadata": {},
   "source": [
    "### Scrapeo de las urls de los ariculos.\n",
    "\n",
    "Mediante esta función revisa cada uno de los links de los artículos en los cuales se encuentra la información\n",
    "del artículo, por lo cual se aplican cada uno de los métodos, que nos van a obtener lo que estamos requiriendo de dicho artículo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2a229-e281-48d0-b37c-d45f926a6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_book(url):\n",
    "    content_book={}\n",
    "    r=requests.get(url)\n",
    "    tr='Revista Mexicana de Análisis de la Conducta'\n",
    "    a='IV. Humanidades y Ciencias de la Conducta'\n",
    "    te='Psicología'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    tituloR=tr\n",
    "    if tituloR:\n",
    "        content_book['Titulo Revista']=tituloR\n",
    "    else :\n",
    "        content_book['Titulo Revista']=None\n",
    "    area=a\n",
    "    if area:\n",
    "        content_book['Area']=area\n",
    "    else :\n",
    "        content_book['Area']=None\n",
    "    tematica=te\n",
    "    if tematica:\n",
    "        content_book['Tematica']=tematica\n",
    "    else :\n",
    "        content_book['Tematica']=None\n",
    "    #titulo del libro\n",
    "    try:\n",
    "        titulo=s_item.find('h3').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titulo\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    try:\n",
    "        des=s_item.find('div', class_='panel-boody').get_text(strip=True)\n",
    "        content_book['Resumen']=des\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    try:\n",
    "        de=s_item.find('div', id='articleAbstract').get_text(strip=True)\n",
    "        content_book['Abstract']=de.replace(\"Abstract\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    try:\n",
    "        urlA=url\n",
    "        content_book['Link Articulo']=urlA\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "        #obtener link PDF\n",
    "    try:\n",
    "        ancla_link=s_item.find('div', id='articleFullText').find('a').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root,ancla_link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9cbf46-5e2c-463d-83ee-4ba8eff8b61e",
   "metadata": {},
   "source": [
    "El list_scraper hace un scrapeo de la cada uno de los artículos, ya que en datos_book se acumuló la información obtenida de lo métodos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6d9e1-8933-4e7f-88b5-447bef66a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=list_scraper[0:248]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d6cb0-0b1a-4375-9718-627d2a0eeb0d",
   "metadata": {},
   "source": [
    "### Se obtiene la información requerida de los artículos.\n",
    "\n",
    "La variable **datos_book** es un listado en el que se recolecto la información requerida por lo cual\n",
    "se convierte en un **DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07826e-e868-4846-9072-fd888f567a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342da385-ac27-4a0f-9ab0-1b4f380f3483",
   "metadata": {},
   "source": [
    "La información solicitada se pasa a un **CSV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c36ff-b52d-456c-9419-fc5cbbfcc6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo.to_csv('Revista050.3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cba7a-7842-4c10-8a1b-81bf19223416",
   "metadata": {},
   "source": [
    "Final del Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe97e87-36ea-48b1-9254-ac9898d75472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15190079-cff5-4b17-b04d-c602bc75881d",
   "metadata": {},
   "source": [
    "# WEB SCRAPING\n",
    "  \n",
    "Es una técnica para extraer y almacenar datos de una o varias páginas web con el fin de analizarlos o manipularlos en otros medios, para la cual se utilizan bots para extraer los datos y contenidos de las webs. \n",
    "\n",
    "### Importación de librerías.\n",
    "\n",
    "* Requests realiza la petición al servidor.\n",
    "\n",
    "* BeautifulSoup analizar documentos HTML.\n",
    "\n",
    "* Pandas podemos representar datos tabulares con columnas con etiquetas y filas y series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4c7aa-8563-4a63-bc53-ccd648ac3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91d5be-308a-4b38-99bc-163478893e30",
   "metadata": {},
   "source": [
    "Se realiza un request de la url_inicial de la revista, url_root nos ayuda a genera un url completa a partir de la url raíz para esto se utiliza la librería urljoin.\n",
    "\n",
    "La revista que se está scrapendo es la siguiente https://polismexico.izt.uam.mx/index.php/rp/about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370a85e-6803-4c21-883b-e31024e10d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial='https://polismexico.izt.uam.mx/index.php/rp/issue/archive?issuesPage=3#issues'\n",
    "url_root= 'https://polismexico.izt.uam.mx/index.php/rp/issue/archive?issuesPage=3#issues'\n",
    "r = requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b91af-7166-44a8-a09d-461cd2219cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fb114-036e-47e7-bc3f-25eb973149a0",
   "metadata": {},
   "source": [
    "### Obtención de los primeros volúmenes.\n",
    "\n",
    "Se realiza la búsqueda para obtener los urls de cada uno de los volúmenes o archivos de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3395cc9-80fc-43f4-820e-85c1ca7763bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = soup.find('div', id='issues')\n",
    "volumen=box.findAll('h4')\n",
    "vol = [x.find('a').get('href')for x in volumen]\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ea2d8-8deb-4de1-beaf-e88053d84e33",
   "metadata": {},
   "source": [
    "### Obtención de los segundos urls.\n",
    "\n",
    "Se realiza una lista en la que la variable *vol* que contiene las urls de cada uno de los volúmenes publicados de la revista, con estos urls obtenidos,  con la variable *vol* se implementa un ciclo en el cual se utiliza la variable *vol2* de la cual se van acumulando las siguientes urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709010d3-9569-49a3-b6eb-03e826a9886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.findAll('div', id='issueCoverImage') \n",
    "    #volumen1=box1.findAll('h3', class_='title')\n",
    "    vol1 = [x.find('a').get('href')for x in box1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0be81-1dcd-47cf-80c0-0da183473025",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa593b-d629-4227-a390-b3d6890281c6",
   "metadata": {},
   "source": [
    "### Obtención de las ulrs de los artículos.\n",
    "\n",
    "Algunas revistas necesitan más búsquedas para llega a los artículos ya que algunos manejan primero los volúmenes o archivos y de eso archivos se dirigen a un urls que es la tabla contenidos, donde se encuentran los artículos, por lo cual se comienza a realizar la misma búsqueda que en *vol2*.\n",
    "\n",
    "Se implementa otro ciclo con *vol2*, para esto se utiliza la variable de *vol3* que recolecta las urls de los artículos, para comenzar con su scrapeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe3b00-f9f4-4547-af9b-05054eee8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3 =[]\n",
    "for b in vol2: \n",
    "    url_inicial2=b \n",
    "    r2 = requests.get(url_inicial2)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('tr', valign='top')\n",
    "    vol2= [o.find('a').get('href')for o in volumen2]\n",
    "    #vol1=[urljoin(url_root,p) for p in vol1]\n",
    "    vol3+=vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447059d-bbf4-4d75-9c3a-fe427b74c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a678f5f-7423-43d9-805e-f15f38e10be6",
   "metadata": {},
   "source": [
    "En esta función variable sopa se utiliza para tener las paginaciones de la revista y la variable url es guardan cada uno de url que ya se obtuvieron de *vol3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1aff7a-6787-4793-9be7-062221af125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_items(sopa, url): \n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('tr', valign='top')\n",
    "    vol2= [o.find('a').get('href')for o in volumen2]\n",
    "    return vol3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd03be-bd8e-4d04-b8ad-1f34d6dcd5b9",
   "metadata": {},
   "source": [
    "Ahora se va acumulando cada uno de los links e ir iterando en cada una de las páginas, para traer cada uno de los links que se van a ir scrapeando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a38a2-d54d-43f6-be52-48806ceb8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break\n",
    "    url_inicial = urljoin(url_inicial, next_a[0].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc92751-e36c-4901-b45c-19909be4e743",
   "metadata": {},
   "source": [
    "Obtiene el número de los artículos que se encontraron en los primeros volúmenes de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c475859-859e-443f-8a9e-f10e3725bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed578b-f3f1-4461-ac54-2da54a85a1f1",
   "metadata": {},
   "source": [
    "Toma uno a uno de los links donde se encuentra la información de cada artículo para Scrapearlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c749f0-ebcf-4acd-a8b5-c3722b69ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629cee1-fc65-48ed-89b0-fb8dbfccccb7",
   "metadata": {},
   "source": [
    "### Scrapeo de las urls de los ariculos.\n",
    "\n",
    "Mediante esta función revisa cada uno de los links de los artículos en los cuales se encuentra la información\n",
    "del artículo, por lo cual se aplican cada uno de los métodos, que nos van a obtener lo que estamos requiriendo de dicho artículo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e46516-bc98-4a28-a75c-fec7b22e4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_book(url):\n",
    "    content_book={}\n",
    "    r=requests.get(url)\n",
    "    tr='Polis'\n",
    "    a='V. Ciencias Sociales'\n",
    "    te='Ciencias Sociales'\n",
    "    ul='http://polismexico.izt.uam.mx/index.php/rp'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    tituloR=tr\n",
    "    if tituloR:\n",
    "        content_book['Titulo Revista']=tituloR\n",
    "    else :\n",
    "        content_book['Titulo Revista']=None\n",
    "    area=a\n",
    "    if area:\n",
    "        content_book['Area']=area\n",
    "    else :\n",
    "        content_book['Area']=None\n",
    "    tematica=te\n",
    "    if tematica:\n",
    "        content_book['Tematica']=tematica\n",
    "    else :\n",
    "        content_book['Tematica']=None\n",
    "    #titulo del libro\n",
    "    try:\n",
    "        titulo=s_item.find('h3').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titulo\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    try:\n",
    "        des=s_item.find('div', id='articleAbstract').get_text(strip=True)\n",
    "        content_book['Resumen']=des.replace(\"Resumen\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    try:\n",
    "        de=s_item.find('div', class_='panel-boody').get_text(strip=True)\n",
    "        content_book['Abstract']=de#.replace(\"Abstract\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    try:\n",
    "        urlA=url\n",
    "        content_book['Link Articulo']=urlA\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "        #obtener link PDF\n",
    "    try:\n",
    "        ancla_link=s_item.find('div', id='articleFullText').find('a').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root,ancla_link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b7eac-b4d2-41a9-8eb3-81e3d7ff09f3",
   "metadata": {},
   "source": [
    "El list_scraper hace un scrapeo de la cada uno de los artículos, ya que en datos_book se acumuló la información obtenida de lo métodos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c5738-1430-4bf7-be50-9d495a03c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=list_scraper[0:90]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683c0b7-6020-4a62-ad62-7e998657cd59",
   "metadata": {},
   "source": [
    "### Se obtiene la información requerida de los artículos.\n",
    "\n",
    "La variable **datos_book** es un listado en el que se recolecto la información requerida por lo cual\n",
    "se convierte en un **DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83ea1a-c0a9-47a3-bf7e-de29da7aad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f895d-5f83-462b-8c23-95da49cc1e54",
   "metadata": {},
   "source": [
    "En el caso de esta revista solo de deseaba obtener los artículos, por lo cual\n",
    "en algunas ocasiones se obtienes cosas diferentes, para esto con la función isin lo que hace es \n",
    "eliminar mediante el **Titulo Articulo** los archivos que no se desean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b169c4-f359-4e4a-b072-6b873dca2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_catalogo[~ df_catalogo[\"Titulo Articulo\"].isin([\"Presentación\",\"Editorial\",\"Autores\",\"Introducción\",\"Prólogo\",\"Comentario\",\"Revista completa\",\"Preliminares\",\"Contenido 1991\",\"Enlaces refback\",\"El problema del humanismo en el Marx maduro\",\"Pura imagen\",\"Número completo\", None,\"Abstracts 2010-I\",\"Contenido Polis\",\"Presentación Polis\",\"Contenido 2010-I\",\"Astracts 2010-II\",\"Presentación 2010-II\",\n",
    "\"Contenido 2000-I\",\"Abstracts 2000-II\",\"Presentación 2000-II\",\n",
    "\"Contenido 2001-I\",\"Abstracts 2001-II\",\"Presentación 2001-II\",\n",
    "\"Contenido 2002-I\",\"Abstracts 2002-II\",\"Presentación 2002-II\",\n",
    "\"Contenido 2003-I\",\"Abstracts 2003-II\",\"Presentación 2003-II\",\n",
    "\"Contenido 2004-I\",\"Abstracts 2004-II\",\"Presentación 2004-II\",\n",
    "\"Contenido 2005-I\",\"Abstracts 2005-II\",\"Presentación 2005-II\",\n",
    "\"Contenido 2006-I\",\"Abstracts 2006-II\",\"Presentación 2006-II\",\"Abstracts 2006-II\",\"Abstracts 2006-I\",\"Presentación 2006-I\",\n",
    "\"Abstracts 2007-II\",\"Abstracts 2007-I\",\"Presentación 2007-I\",\"Presentación 2007-II\",\n",
    "\"Abstracts 2008-II\",\"Abstracts 2008-I\",\"Presentación 2009-I\",\"Presentación 2009-II\",\n",
    "\"Abstracts 2009-II\",\"Abstracts 2009-I\",\"Presentación 2014-I\",\"Presentación 2014-II\",\n",
    "\"Abstracts 2002-II\",\"Abstracts 2003-I\",\"Presentación 2009-I\",\"Presentación 2009-II\",\n",
    "\"Abstracts 2004-II\",\"Abstracts 2004-I\",\"Presentación 2002-I\",\"Presentación 2002-II\",\n",
    "\"Abstracts 2003-II\",\"Abstracts 2003-I\",\"Presentación 2001-I\",\"Presentación 2002-II\",\n",
    "\"Abstracts 2002-II\",\"Abstracts 2002-I\",\"Presentación 2002-I\",\"Presentación 2002-II\",\n",
    "\"Polis. México 2002-I\",\"Descargar número\",\"Contenido 2003-II\",\"Contenido  2004-I\",\"Abstracts\",\"Contenido\"])]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea963c98-0a82-4b8f-ad5d-a3852a8a0608",
   "metadata": {},
   "source": [
    "La información solicitada se pasa a un **CSV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a24ee7-2fea-4ef2-9113-f1ab7f6b470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Revista198.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a569857-b9b5-4491-b307-d2695810b80f",
   "metadata": {},
   "source": [
    "Final del Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da805d-3d6b-4157-a835-4332bc07263d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

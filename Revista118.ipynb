{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0ccec-9ad3-4505-a741-570557f24360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dedb8-60d6-4b6a-9f5c-f5687dfcd427",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial='https://ref.uabc.mx/ojs/index.php/ref/issue/archive/3'\n",
    "url_root= 'https://ref.uabc.mx/ojs/index.php/ref/issue/archive/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c99633-500e-43c7-a4a8-66f93db71d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370046f9-00f1-4d32-9290-23e480511ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6560ae5-a8aa-4401-9fef-497f866059b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = soup.find('div', class_='page page_issue_archive')\n",
    "volu=box.findAll('h2')\n",
    "vol = [x.find('a').get('href')for x in volu]\n",
    "#vol=[urljoin(url_root,i) for i in vol]\n",
    "vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5680e-3f5d-4061-8547-c3da9812f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.find('div', class_='sections') \n",
    "    volumen1=box1.findAll('div', class_='obj_article_summary')\n",
    "    vol1 = [x.find('a').get('href')for x in volumen1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487b523-e6af-4344-9c31-07d5246f9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d35e33-0cac-4076-a16a-71f4a3f07846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_items(sopa, url): \n",
    "    box1 = soup1.find('div', class_='sections') \n",
    "    volumen1=box1.findAll('div', class_='obj_article_summary')\n",
    "    vol1 = [x.find('a').get('href')for x in volumen1]\n",
    "    #vol=[urljoin(url_root,i) for i in vol]\n",
    "    return vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2245c2a-59ef-4dd8-86f4-dd456a1ba94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break\n",
    "    url_inicial = urljoin(url_inicial, next_a[0].get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667995c-26eb-4505-9061-b19db456878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9edc803-9b66-4c49-b31f-f562005e5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace20548-7284-4e11-94e8-a180c387f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_book(url):\n",
    "    content_book={}\n",
    "    r=requests.get(url)\n",
    "    tr='Estudios Fronterizos'\n",
    "    a='V. Ciencias Sociales'\n",
    "    te='Sociología, Antropología, Historia, Economía, Demografía, Geografía Humana, Comunicación y Relaciones Internacionales.'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    tituloR=tr\n",
    "    if tituloR:\n",
    "        content_book['Titulo Revista']=tituloR\n",
    "    else :\n",
    "        content_book['Titulo Revista']=None\n",
    "    area=a\n",
    "    if area:\n",
    "        content_book['Area']=area\n",
    "    else :\n",
    "        content_book['Area']=None\n",
    "    tematica=te\n",
    "    if tematica:\n",
    "        content_book['Tematica']=tematica\n",
    "    else :\n",
    "        content_book['Tematica']=None\n",
    "    #titulo del libro\n",
    "    try:\n",
    "        titulo=s_item.find('font', size='4').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titulo\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    try:\n",
    "        des=s_item.find('section', class_='item abstract').text.split(\"Resumen\")[1]\n",
    "        content_book['Resumen']=des.strip()#replace(\"Resumen\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    try:\n",
    "        de=s_item.find('section', class_='item abstract').text.split(\"Abstract\")[1]\n",
    "        content_book['Abstract']=de.strip()#replace(\"Abstract\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    try:\n",
    "        urlA=url\n",
    "        content_book['Link Articulo']=urlA\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "        #obtener link PDF\n",
    "    try:\n",
    "        ancla_link=s_item.find('font', size='1').find('a').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root,ancla_link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70e501-7d38-4ef4-9a8b-0edefbca5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=list_scraper[0:125]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec501888-2edc-4122-be3d-4f5eb412ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5e447-5099-4c64-adf0-068b4b7ce6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo.to_csv('Revista118.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5b32b-464a-49b2-9807-72bd229c8437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
